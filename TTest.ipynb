{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B test\n",
    "## T-Tests and P-Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we're running an A/B test. We'll fabricate some data that randomly assigns order amounts from customers in sets A and B, with B being a little bit higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-14.401489079766488, pvalue=8.683098950371365e-47)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#A - test group\n",
    "A = np.random.normal(25.0, 5.0, 10000)\n",
    "#B - control group\n",
    "B = np.random.normal(26.0, 5.0, 10000)\n",
    "\n",
    "#running test in box\n",
    "stats.ttest_ind(A, B)\n",
    "#negative statistics means changes have negative impact\n",
    "#low pvalue means that the result is real (not occasional deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-statistic is a measure of the difference between the two sets expressed in units of standard error. Put differently, it's the size of the difference relative to the variance in the data. A high t value means there's probably a real difference between the two sets; you have \"significance\". The P-value is a measure of the probability of an observation lying at extreme t-values; so a low p-value also implies \"significance.\" If you're looking for a \"statistically significant\" result, you want to see a very low p-value and a high t-statistic (well, a high absolute value of the t-statistic more precisely). In the real world, statisticians seem to put more weight on the p-value result.\n",
    "\n",
    "Let's change things up so both A and B are just random, generated under the same parameters. So there's no \"real\" difference between the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-0.5228944122460242, pvalue=0.6010535263082546)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.random.normal(25.0, 5.0, 10000)\n",
    "\n",
    "stats.ttest_ind(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our t-statistic is much lower and our p-value is really high. This supports the null hypothesis - that there is no real difference in behavior between these two sets.\n",
    "\n",
    "Let's do the same thing - where the null hypothesis is accurate - but with 10X as many samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.4805328626808123, pvalue=0.1387326725970262)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.normal(25.0, 5.0, 100000)\n",
    "B = np.random.normal(25.0, 5.0, 100000)\n",
    "\n",
    "stats.ttest_ind(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our p-value actually got a higher, and the t-test smaller, but still not enough to declare a real difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.5599442298496917, pvalue=0.5755175413543256)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.normal(25.0, 5.0, 1000000)\n",
    "B = np.random.normal(25.0, 5.0, 1000000)\n",
    "\n",
    "stats.ttest_ind(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we increase samples again, we got lower statistics (less influence) and higher pvalue (test in not relevant).\n",
    "\n",
    "So, you could have reached the right decision with just 10,000 samples instead of 100,000. Even a million samples doesn't help.\n",
    "\n",
    "There is no significant difference between between A & B\n",
    "\n",
    "If we compare the same set to itself, by definition we get a t-statistic of 0 and p-value of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.0, pvalue=1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(A, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run A/A test to check if we have system bugs. A/A test shows we can trust our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the influence of std on the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-0.6387370726739928, pvalue=0.5229947225536924)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.normal(25.0, 5.0, 100000)\n",
    "B = np.random.normal(25.0, 6.0, 100000)\n",
    "\n",
    "stats.ttest_ind(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our p-value is low, and the t-test high.\n",
    "\n",
    "Let's increase std difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.9886650911938444, pvalue=0.3228282238988791)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.normal(25.0, 5.0, 100000)\n",
    "B = np.random.normal(25.0, 10.0, 100000)\n",
    "\n",
    "stats.ttest_ind(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our p-value got higher, and the t-test bit smaller, but not enough to declare influence.\n",
    "\n",
    "Let's increase std difference more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.0366643223315852, pvalue=0.2998935782223358)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.normal(25.0, 5.0, 100000)\n",
    "B = np.random.normal(25.0, 20.0, 100000)\n",
    "\n",
    "stats.ttest_ind(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our p-value got bit higher, and the t-test bit smaller.\n",
    "We see that with the same center & different std we get randomly different small result in statistics.\n",
    "It means we have no seriouse changes in result.\n",
    "Higher the difference in std shows us that the result is less random.\n",
    "\n",
    "In our case we still can declare no significant influence.\n",
    "\n",
    "Let's try increase both std in very first case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-5.825832749506706, pvalue=5.770018341440199e-09)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.normal(25.0, 10.0, 10000)\n",
    "B = np.random.normal(26.0, 10.0, 10000)\n",
    "\n",
    "stats.ttest_ind(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-4.345954711297113, pvalue=1.3935513465654028e-05)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.normal(25.0, 5.0, 10000)\n",
    "B = np.random.normal(26.0, 20.0, 10000)\n",
    "\n",
    "stats.ttest_ind(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have difference in centers, we see it in satistics. \n",
    "Though higher std or difference in std, we got less stable result and p-value shows less negative impact.\n",
    "P-value in both cases is very small. It means in both cases we have negative impact we can declare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
